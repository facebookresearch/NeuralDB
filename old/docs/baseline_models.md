## Baseline (end-to-end) models

For the baseline models, they can be fine-tuned with the following command. Setting the DB version, retriever and size environment variables.

There's a few different variants of the experiments that can be run, trading off the database size and the retriever used.

* Inline retrievers: TF-IDF, BM25 `--retriever=tfidf` or `--retriever=bm25`
* External (pipeline) retrievers: DPR `--retriever=pipeline`
* Oracle (perfect) retriever `--retriever=all` or `--oracle`
* No retriever (whole database) `--retriever=all`

The database sizes are: 50,100,500,1000,2000,5000,7000,10000

### Technical notes

#### TF-IDF and BM25
These are fairly cheap to compute. The TF-IDF and BM25 
retrievers are run as part of the dataset reader.

See the following files. These are just extended versions of the DrQA implementation.

``` 
src/neuraldb/dataset/search_engines/bm25.py
src/neuraldb/dataset/search_engines/tfidf.py
```

These search methods work well when there's a lot of token overlap. But not too great for multihop. 
A good failure mode is for queries such as "who is the oldest" where TF-IDF can't retrieve "John smith was born in 1929"

#### DPR / Pipeline

Majid ran the DPR retriever on the dataset TODO add link to repo

The DPR predictions (for a DB of 50 facts) are available in this [Google drive](https://drive.google.com/drive/folders/1TtEJobhKFQ6bIthd-2jApBYTxZQyV4qV?usp=sharing)

There is a separate reader that uses a `pipeline` retrieved evidence instead of running the search engine. This could be used for _any_ preprocessed retrieval data:

```
--retriever=pipeline
```

The format is a list of (query, answer) inputs with an additional field `context_score` for retrieved facts. This assumes an ordered list of facts with scores.  
```
[
{
    "question": "Is Elizabeth serious?", 
    "context_height": 1, 
    "gold_facts": [1], 
    "answer": "None", 
    "metadata": {"relation_type": "is", "query_type": "atomic_boolean"}, 
    "context-score": [[0, "Elizabeth works at Consolidated Mercantile Inc.. Elizabeth is a writer.", 52.7545166015625]]
},
...
]
```

#### WholeDB 

Another baseline experiment is encoding the entire database. This is limited to approximately 50 facts because of the limitations 
of encoding large inputs into transformer models.

Note that for `all`, the options should change (and perhaps smaller batch size is needed for P100 GPUs.)
```
--max_target_length=256 \
--max_source_length=1024
```

#### Perfect IR / Oracle
The `--oracle` flag changes the dataset reader to not use the search engine and instead just use gold labeled instances.
This only affects `v0.5` of the dataset which has these annotations included.

### Training

```bash
export PYTHONPATH=src:abstractive-models:extractive-models
version=v0.5
size=50
retriever=tfidf

python -m neuraldb.commands.finetune_end2end \
--output_dir ./output/e2e/version=$version,size=$size,retriever=$retriever \
--model_name_or_path t5-base \
--do_train \
--train_batch_size 8 \
--eval_batch_size 8 \
--n_gpu 1 \
--learning_rate=1e-3 \
--num_train_epochs 5 \
--train_path=$version/train_queries_last_$size.json \
--val_path=$version/dev_queries_last_$size.json \
--dataset_version=$version \
--retriever=$retriever
```


### Fine-tuning fusion in decoder

FiD uses a different model and generator implementation to the end-to-end model. Instead of merging facts into a single instance, it creates multiple fact-instance pairs. 
To run FiD, use the following command:

```bash
export PYTHONPATH=src:abstractive-models:extractive-models
version=v0.5
size=50
retriever=tfidf

python -m neuraldb.commands.finetune_end2end_fusion \
--output_dir ./output/e2e_fid/version=$version,size=$size,retriever=$retriever \
--model_name_or_path t5-base \
--do_train \
--train_batch_size 8 \
--eval_batch_size 8 \
--n_gpu 1 \
--learning_rate=1e-3 \
--num_train_epochs 5 \
--train_path=$version/train_queries_last_$size.json \
--val_path=$version/dev_queries_last_$size.json \
--dataset_version=$version \
--retriever=$retriever
```

Again, the same option for `--oracle` holds, but Pipeline hasn't been implemented for this case.


### Prediction

```bash
export PYTHONPATH=src:abstractive-models:extractive-models
version=v0.5
size=50
retriever=tfidf

python -m neuraldb.commands.finetune_end2end \
--output_dir ./output/e2e/version=$version,size=$size,retriever=$retriever \
--model_name_or_path t5-base \
--do_predict \
--eval_batch_size 8 \
--n_gpu 1 \
--train_path=$version/train_queries_last_$size.json \
--val_path=$version/dev_queries_last_$size.json \
--test_path=$version/test_queries_last_$size.json \
--dataset_version=$version \
--retriever=$retriever
```

The metrics_test.json file generated by the prediction script will contain the scores for each instance (it wouldn't be too hard to extend this to output the predictions too). The script will make a breakdown by relation and question type for scoring too.


### Evaluation

The metrics_test.json file will have all evaluation info and score breakdowns for the instance. 

