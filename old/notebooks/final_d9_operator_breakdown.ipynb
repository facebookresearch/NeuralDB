{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append(\"../src\")\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from neuraldb.scoring.r_precision import f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "search_root = \"/checkpoint/jth/job_staging/neuraldb_expts/experiment=oracle_d3\"\n",
    "checkpoint_name = \"metrics_test.json\"\n",
    "files = glob(\"{}*/**/{}\".format(search_root,checkpoint_name), recursive=True)\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def expand(idx,chunk):\n",
    "  #elif idx == 1:\n",
    "  #  return [\"experiment={}\".format(chunk)]\n",
    "  if chunk.startswith(\"seed-\"):\n",
    "    return [\"seed={}\".format(chunk.replace(\"seed-\",\"\"))]\n",
    "  elif \",\" in chunk:\n",
    "    return chunk.split(\",\")\n",
    "  elif \"=\" in chunk:\n",
    "    return [chunk]\n",
    "\n",
    "  return []\n",
    "\n",
    "experiments = []\n",
    "for file in files:\n",
    "    chunks = file.split(\"/\")\n",
    "    chunks = itertools.chain(*[expand(idx, chunk) for idx, chunk in enumerate(chunks)])\n",
    "\n",
    "    data = {k:v for k,v in (chunk.split(\"=\") for chunk in chunks)}\n",
    "    data[\"file\"] = file\n",
    "    data['dir'] = os.path.dirname(file)\n",
    "    experiments.append(data)\n",
    "\n",
    "\n",
    "print(len(experiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "for experiment in tqdm(experiments):\n",
    "    all_raw = []\n",
    "    em = 0.0\n",
    "    with open(experiment['file']) as f:\n",
    "        for line in f:\n",
    "            partial_results = json.loads(line)\n",
    "            all_raw.extend(partial_results['test']['raw'])\n",
    "\n",
    "\n",
    "    experiment[\"EM\"] = np.mean([rec[2] for rec in all_raw])\n",
    "    experiment[\"raw\"] = all_raw\n",
    "\n",
    "    gold = defaultdict(lambda: defaultdict(list))\n",
    "    for instance in experiment[\"raw\"]:\n",
    "        if instance[0] != \"[NULL_ANSWER]\" or instance[1] != \"[NULL_ANSWER]\":\n",
    "\n",
    "            gold[instance[3][\"query_type\"]][instance[3][\"query\"][\"input\"]].append((instance[0], instance[1]))\n",
    "\n",
    "\n",
    "    aem = 0\n",
    "    aem_count = 0\n",
    "\n",
    "\n",
    "    scores = defaultdict(int)\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for t, questions in gold.items():\n",
    "        \n",
    "        if t in {\"atomic_boolean\",\"join_boolean\", \"atomic_extractive\",\"join_extractive\"}:\n",
    "            for question, answers in questions.items():\n",
    "                print(answers)\n",
    "                for answer in answers:\n",
    "                    aem_count +=1\n",
    "                    counts[t] += 1\n",
    "\n",
    "                    if answer[0] == answer[1]:\n",
    "                        aem += 1\n",
    "                        scores[t]+=1\n",
    "        elif t == \"min/max\":\n",
    "\n",
    "            for question, answers in questions.items():\n",
    "\n",
    "                argmin_aggr_gold = defaultdict(list)\n",
    "                argmin_aggr_pred = defaultdict(list)\n",
    "                for answer in answers:\n",
    "                    if answer[0] != \"[NULL_ANSWER]\" and \"[LIST]\" in answer[0]:\n",
    "                        key,value = answer[0].split(\"[LIST]\",maxsplit=1)\n",
    "                        argmin_aggr_pred[key.strip()] = value.strip()\n",
    "\n",
    "                    if answer[1] != \"[NULL_ANSWER]\":\n",
    "                        key,value = answer[1].split(\"[LIST]\",maxsplit=1)\n",
    "                        argmin_aggr_gold[key.strip()] = value.strip()\n",
    "\n",
    "                min_item_gold = sorted(argmin_aggr_gold.items(),key=lambda item: len(item[1]))\n",
    "                min_item_pred = sorted(argmin_aggr_pred.items(),key=lambda item: len(item[1]))\n",
    "\n",
    "                max_item_gold = sorted(argmin_aggr_gold.items(),key=lambda item: len(item[1]),reverse=True)\n",
    "                max_item_pred = sorted(argmin_aggr_pred.items(),key=lambda item: len(item[1]),reverse=True)\n",
    "\n",
    "                aem_count +=1\n",
    "                counts[t] +=1\n",
    "                if len(min_item_pred) and len(max_item_pred):\n",
    "                    if min_item_gold[0][0] == min_item_pred[0][0] or max_item_gold[0][0] == max_item_pred[0][0]:\n",
    "                        aem+=1\n",
    "                        scores[t]+=1\n",
    "\n",
    "\n",
    "        elif t == \"set\":\n",
    "\n",
    "            for question, answers in questions.items():\n",
    "                set_gold = set()\n",
    "                set_pred = set()\n",
    "                for answer in answers:\n",
    "                    if answer[0] != \"[NULL_ANSWER]\":\n",
    "                        set_pred.add(answer[0].strip())\n",
    "\n",
    "                    if answer[1] != \"[NULL_ANSWER]\":\n",
    "                        set_gold.add(answer[1].strip())\n",
    "\n",
    "                aem_count +=1\n",
    "                counts[t] +=1\n",
    "                aem += f1(set_gold, set_pred)\n",
    "                scores[t] += f1(set_gold, set_pred)\n",
    "\n",
    "\n",
    "        elif t == \"count\":\n",
    "            for question, answers in questions.items():\n",
    "\n",
    "                set_gold = set()\n",
    "                set_pred = set()\n",
    "                for answer in answers:\n",
    "                    if answer[0] != \"[NULL_ANSWER]\":\n",
    "                        set_pred.add(answer[0].strip())\n",
    "\n",
    "                    if answer[1] != \"[NULL_ANSWER]\":\n",
    "                        set_gold.add(answer[1].strip())\n",
    "\n",
    "                aem_count +=1\n",
    "                aem += 1 if len(set_gold) == len(set_pred) else 0\n",
    "                scores[t] += 1 if len(set_gold) == len(set_pred) else 0\n",
    "                counts[t] += 1\n",
    "\n",
    "\n",
    "\n",
    "    for k,v in counts.items():\n",
    "        experiment[\"A_type_{}\".format(k)] = scores[k]/v\n",
    "\n",
    "    experiment[\"A_EM\"] =aem/aem_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             A_EM A_type_atomic_boolean  \\\n                             mean                  mean   \nexperiment model   lr                                     \noracle_d3  t5-base 4e-4  0.987229               0.98673   \n\n                        A_type_atomic_extractive A_type_count  \\\n                                            mean         mean   \nexperiment model   lr                                           \noracle_d3  t5-base 4e-4                 0.984221      0.98773   \n\n                        A_type_join_boolean A_type_join_extractive  \\\n                                       mean                   mean   \nexperiment model   lr                                                \noracle_d3  t5-base 4e-4            0.989637               0.988095   \n\n                        A_type_min/max A_type_set        EM  \n                                  mean       mean      mean  \nexperiment model   lr                                        \noracle_d3  t5-base 4e-4            1.0   0.998208  0.985377  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>A_EM</th>\n      <th>A_type_atomic_boolean</th>\n      <th>A_type_atomic_extractive</th>\n      <th>A_type_count</th>\n      <th>A_type_join_boolean</th>\n      <th>A_type_join_extractive</th>\n      <th>A_type_min/max</th>\n      <th>A_type_set</th>\n      <th>EM</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n      <th>mean</th>\n    </tr>\n    <tr>\n      <th>experiment</th>\n      <th>model</th>\n      <th>lr</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>oracle_d3</th>\n      <th>t5-base</th>\n      <th>4e-4</th>\n      <td>0.987229</td>\n      <td>0.98673</td>\n      <td>0.984221</td>\n      <td>0.98773</td>\n      <td>0.989637</td>\n      <td>0.988095</td>\n      <td>1.0</td>\n      <td>0.998208</td>\n      <td>0.985377</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(experiments).fillna(0)\n",
    "\n",
    "cols = {col:[np.mean] for col in filter(lambda col: col == \"A_EM\" or col == \"EM\" or col.startswith(\"A_\") or col.startswith(\"prop_\") or col.startswith(\"type_\"),results.columns)}\n",
    "cols.update({col:[np.max] for col in filter(lambda col: col.startswith(\"count_type_negative\"), results.columns)})\n",
    "breakdown_cols = list(filter(lambda col: col.startswith(\"prop_\"),results.columns))\n",
    "type_cols = list(filter(lambda col: (col.startswith(\"type_\") and \"negative\" not in col) or col.startswith(\"x\"),results.columns))\n",
    "print(type_cols)\n",
    "type_cols2 = list(filter(lambda col: col.startswith(\"type_\") and \"negative\" not in col or col == \"x_avg_negative\",results.columns))\n",
    "a_type_cols = list(filter(lambda col: col.startswith(\"A_type_\") and \"negative\" not in col or col == \"x_avg_negative\",results.columns))\n",
    "type_cols3 = list(filter(lambda col: \"count\" not in col and  \"negative\" in col,results.columns))\n",
    "type_cols4 = list(filter(lambda col: col.startswith(\"type_\") and \"negative\" not in col,results.columns))\n",
    "breakdown = pd.pivot_table(results, index=[\"experiment\",\"model\",\"lr\"],columns=[],aggfunc=cols)\n",
    "#pd.option_context(\"display.max_rows\",None)\n",
    "pd.options.display.max_rows = 150\n",
    "breakdown\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "neuraldb",
   "language": "python",
   "display_name": "neuraldb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}