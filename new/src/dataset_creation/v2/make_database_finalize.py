import json
import random
from argparse import ArgumentParser
from collections import Counter, defaultdict

import numpy as np
from tqdm import tqdm

from dataset_creation.v2.make_database_initial import normalize_subject
from wikidata_common.kelm import KELMMongo
from wikidata_common.wikidata import Wikidata

# This will only return S,R,O triples that match our templates
def generate_hypotheses(instance):
    for s,r,o in instance['valid_hypotheses']:
        if r not in final_templates:
            continue
        yield (s,r,o)


# This goes through the instances and pulls in related facts for joins/complex Qs
def bring_extra_facts(instance, extra, additional_list, search_id, is_subj):
    # If it's subject, it's the first element of the tuple, if object it's the last.
    pid = 0 if is_subj else 2

    # Iterate through all found KELM-mapped instances.
    # Each relation contains a set of the relation IDs and valid hypotheses
    for found in extra:

        # If it's the same fact. skip
        if found['reference'] == instance['reference']:
            continue

        # Check if the KELM instance contains a relation that's in the additional list
        filtered = list(filter(lambda hyp: hyp[pid] == search_id and hyp[1] in additional_list, found["valid_hypotheses"]))

        if not any(filtered):
            continue

        # If we're adding duplicate facts, then we're undoing the need for joins. so skip
        if any([f in instance['valid_hypotheses'] for f in filtered]):
            continue


        yield found


if __name__ == "__main__":
    kelm = KELMMongo()
    wiki = Wikidata()
    global_obs = []

    parser = ArgumentParser()
    parser.add_argument("in_file")
    parser.add_argument("out_file")
    args = parser.parse_args()

    with open("generate_v1.5.json") as f:
        final_templates = json.load(f)

    with open("filter_subjects.json") as f:
        additional_subjects = json.load(f)

    with open("filter_objects.json") as f:
        additional_objects = json.load(f)

    with open("expand_objects.json") as f:
        extra_objects = json.load(f)

    with open("expand_subject.json") as f:
        extra_subjects = json.load(f)

    original_for = defaultdict(list)
    extra_kelm_for = defaultdict(list)

    # Go through all databases in the input file
    with open(args.in_file) as f, open(args.out_file,"w+") as of:
        for line in tqdm(f):
            db = json.loads(line)
            local_obs = []

            # For all facts in the database
            for instance in tqdm(db):
                for hyp in instance['valid_hypotheses']:
                    hyp = tuple(hyp)
                    subject_id, relation_id, object_id = hyp

                    # Find an extra subject
                    extra_subj = list(kelm.find_entity_rel(subject_id, set(additional_subjects.get(relation_id,{}).keys()).union(extra_subjects.get(relation_id,{}).keys())))

                    # Find an extra object
                    if object_id.startswith("Q"):
                        extra_obj = list(kelm.find_entity_rel(object_id, set(additional_objects.get(relation_id,{}).keys()).union(extra_objects.get(relation_id,{}).keys())))
                    else:
                        extra_obj = []
                    #
                    # print(len(extra_subj), len(extra_obj))

                    # Bring facts for these extras
                    ex_s = bring_extra_facts(instance, extra_subj, set(additional_subjects.keys()).union(extra_subjects.keys()), subject_id, True)
                    ex_o = bring_extra_facts(instance, extra_obj, set(additional_objects.keys()).union(extra_objects.keys()), object_id, True)

                    for e in ex_s:
                        extra_kelm_for[hyp].append(e)

                    for e in ex_o:
                         extra_kelm_for[hyp].append(e)

                    modifiers = []
                    local_obs.append(hyp[1])
                    original_for[hyp].append(instance)


            global_obs.append(local_obs)
            target_size = 25

            # Sample extra facts for a subset. limit to maximum of 1/3 of the size of the dataset
            extra_key = list(extra_kelm_for.keys())
            sampled_extra_facts = random.sample(extra_key,k=min(len(extra_key), target_size//3))


            sampled = []
            references = set()
            for f in sampled_extra_facts:
                sampled.extend(original_for[f])
                extra_item = random.choice(extra_kelm_for[f])
                sampled.append(extra_item)

            # Go through all retrieved facts.
            # First, deduplicate, then normalize the subjects, then add it to the DB
            duplicates = []
            for idx,sampled_fact in enumerate(sampled):
                if 'fact' not in sampled_fact:
                    fact = sampled_fact['candidate'].strip()
                    for subj, rel, obj in sampled_fact['valid_hypotheses']:
                        if obj == "numeric":
                            break

                        name = wiki.get_by_id_or_uri(subj)['english_name']
                        fact = normalize_subject(name, fact)

                        if fact is None:
                            # print("Skipped as unparasable - subj")
                            break

                        if obj.startswith("Q"):
                            name = wiki.get_by_id_or_uri(obj)['english_name']
                            fact = normalize_subject(name, fact)

                        if fact is None:
                            # print("Skipped as unparasable - obj")
                            break

                    sampled_fact['fact'] = fact
                    if fact is None or "‚Åá" in fact:
                        duplicates.append(idx)

                if sampled_fact['reference'] in references:
                    duplicates.append(idx)

                if sampled_fact['fact'] is None:
                    duplicates.append(idx)
                references.add(sampled_fact['reference'])

            # Delete duplicates
            duplicates = list(set(duplicates))
            for dup in sorted(duplicates,reverse=True):
                del sampled[dup]

            # Add unqiue extra facts
            others = []
            for item in db:
                if item['reference'] not in references:
                    others.append(item)
            sampled.extend(random.sample(others,k=min(len(others),target_size-len(sampled))))


            # Remove object IDs from sampled
            for s in sampled:
                if '_id' in s:
                    del s["_id"]



            of.write(json.dumps({"metadata": {"raw":sampled}, "facts":[f['fact'] for f in sampled]})+"\n")


    rel_avgs = defaultdict(list)
    for observation in global_obs:
        ctr = Counter()
        ctr.update(observation)

        for k,v in ctr.items():
            rel_avgs[k].append(v)


    for k,v in rel_avgs.items():
        print(k,np.mean(v), np.std(v))
